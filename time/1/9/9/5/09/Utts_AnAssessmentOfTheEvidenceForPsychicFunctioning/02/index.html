<!--#include virtual="/header-start.html" -->
<title>Science Notes</title>
<meta content="Utts, Jessica" name="author"/>
<meta content="Utts, Jessica" name="copyright"/>
<link href=".." rel="start" title="An Assessment Of The Evidence For Psychic Functioning">
<link href="../01" rel="prev" title="Introduction">
<link href="../03" rel="next" title="Science Notes">
<!--#include virtual="/header-end.html" -->
<h3>Definitions and Research Procedures</h3>
<p>There are two basic types of functioning that are generally considered under the broad heading of psychic or
  paranormal abilities. These are classically known as extrasensory perception (ESP), in which one acquires information
  through unexplainable means and psychokinesis, in which one physically manipulates the environment through unknown
  means. The SAIC laboratory uses more neutral terminology for these abilities; they refer to ESP as anomalous cognition
  (AC) and to psychokinesis as anomalous perturbation (AP). The vast majority of work at both SRI and SAIC investigated
  anomalous cognition rather than anomalous perturbation, although there was some work done on the latter.</p>
<p>Anomalous cognition is further divided into categories based on the apparent source of the information. If it appears
  to come from another person, the ability is called telepathy, if it appears to come in real time but not from another
  person it is called clairvoyance and if the information could have only been obtained by knowledge of the future, it
  is called precognition.</p>
<p>It is possible to identify apparent precognition by asking someone to describe something for which the correct answer
  isn't known until later in time. It is more difficult to rule out precognition in experiments attempting to test
  telepathy or clairvoyance, since it is almost impossible to be sure that subjects in such experiments never see the
  correct answer at some point in the future. These distinctions are important in the quest to identify an explanation
  for anomalous cognition, but do not bear on the existence issue.</p>
<p>The vast majority of anomalous cognition experiments at both SRI and SAIC used a technique known as remote viewing.
  In these experiments, a viewer attempts to draw or describe (or both) a target location, photograph, object or short
  video segment. All known channels for receiving the information are blocked. Sometimes the viewer is assisted by a
  monitor who asks the viewer questions; of course in such cases the monitor is blind to the answer as well. Sometimes a
  sender is looking at the target during the session, but sometimes there is no sender. In most cases the viewer
  eventually receives feedback in which he or she learns the correct answer, thus making it difficult to rule out
  precognition as the explanation for positive results, whether or not there was a sender.</p>
<p>Most anomalous cognition experiments at SRI and SAIC were of the free-response type, in which viewers were simply
  asked to describe the target. In contrast, a forced-choice experiment is one in which there are a small number of
  known choices from which the viewer must choose. The latter may be easier to evaluate statistically but they have been
  traditionally less successful than free-response experiments. Some of the work done at SAIC addresses potential
  explanations for why that might be the case.</p>
<section>
  <h3>Statistical Issues and Definitions</h3>
  <p>Few human capabilities are perfectly replicable on demand. For example, even the best hitters in the major baseball
    leagues cannot hit on demand. Nor can we predict when someone will hit or when they will score a home run. In fact,
    we cannot even predict whether or not a home run will occur in a particular game. That does not mean that home runs
    don't exist.</p>
  <p>Scientific evidence in the statistical realm is based on replication of the same average performance or
    relationship over the long run. We would not expect a fair coin to result in five heads and five tails over each set
    of ten tosses, but we can expect the proportion of heads and tails to settle down to about one half over a very long
    series of tosses. Similarly, a good baseball hitter will not hit the ball exactly the same proportion of times in
    each game but should be relatively consistent over the long run.</p>
  <p>The same should be true of psychic functioning. Even if there truly is an effect, it may never be replicable on
    demand in the short run even if we understand how it works. However, over the long run in well-controlled laboratory
    experiments we should see a consistent level of functioning, above that expected by chance. The anticipated level of
    functioning may vary based on the individual players and the conditions, just as it does in baseball, but given
    players of similar ability tested under similar conditions the results should be replicable over the long run. In
    this report we will show that replicability in that sense has been achieved.</p>
  <section>
    <h4>2.2.1 P-values and Comparison with Chance.</h4>
    <p>In any area of science, evidence based on statistics comes from comparing what actually happened to what should
      have happened by chance. For instance, without any special interventions about 51 percent of births in the United
      States result in boys. Suppose someone claimed to have a method that enabled one to increase the chances of having
      a baby of the desired sex. We could study their method by comparing how often births resulted in a boy when that
      was the intended outcome. If that percentage was higher than the chance percentage of 51 percent over the long
      run, then the claim would have been supported by statistical evidence.</p>
    <p>Statisticians have developed numerical methods for comparing results to what is expected by chance. Upon
      observing the results of an experiment, the p-value is the answer to the following question: If chance alone is
      responsible for the results, how likely would we be to observe results this strong or stronger? If the answer to
      that question, i.e. the p-value is very small, then most researchers are willing to rule out chance as an
      explanation. In fact it is commonly accepted practice to say that if the p-value is 5 percent (0.05) or less, then
      we can rule out chance as an explanation. In such cases, the results are said to be statistically significant.
      Obviously the smaller the p-value, the more convincingly chance can be ruled out.</p>
    <p>Notice that when chance alone is at work, we erroneously find a statistically significant result about 5 percent
      of the time. For this reason and others, most reasonable scientists require replication of non-chance results
      before they are convinced that chance can be ruled out.</p>
  </section>
  <section>
    <h4>Replication and Effect Sizes</h4>
    <p>In the past few decades scientists have realized that true replication of experimental results should focus on
      the magnitude of the effect, or the effect size rather than on replication of the p-value. This is because the
      latter is heavily dependent on the size of the study. In a very large study, it will take only a small magnitude
      effect to convincingly rule out chance. In a very small study, it would take a huge effect to convincingly rule
      out chance.</p>
    <p>In our hypothetical sex-determination experiment, suppose 70 out of 100 births designed to be boys actually
      resulted in boys, for a rate of 70 percent instead of the 51 percent expected by chance. The experiment would have
      a p-value of 0.0001, quite convincingly ruling out chance. Now suppose someone attempted to replicate the
      experiment with only ten births and found 7 boys, i.e also 70 percent. The smaller experiment would have a p-value
      of 0.19, and would not be statistically significant. If we were simply to focus on that issue, the result would
      appear to be a failure to replicate the original result, even though it achieved exactly the same 70 percent boys!
      In only ten births it would require 90 percent of them to be boys before chance could be ruled out. Yet the 70
      percent rate is a more exact replication of the result than the 90 percent.</p>
    <p>Therefore, while p-values should be used to assess the overall evidence for a phenomenon, they should not be used
      to define whether or not a replication of an experimental result was "successful." Instead, a successful
      replication should be one that achieves an effect that is within expected statistical variability of the original
      result, or that achieves an even stronger effect for explainable reasons.</p>
    <p>A number of different effect size measures are in use in the social sciences, but in this report we will focus on
      the one used most often in remote viewing at SRI and SAIC. Because the definition is somewhat technical it is
      given in Appendix 1. An intuitive explanation will be given in the next subsection. Here, we note that an effect
      size of 0 is consistent with chance, and social scientists have, by convention, declared an effect size of 0.2 as
      small, 0.5 as medium and 0.8 as large. A medium effect size is supposed to be visible to the naked eye of a
      careful observer, while a large effect size is supposed to be evident to any observer.</p></section>
  <section>
    <h4>Randomness and Rank-Order Judging</h4>
    <p>At the heart of any statistical method is a definition of what should happen "randomly" or "by chance." Without a
      random mechanism, there can be no statistical evaluation.</p>
    <p>There is nothing random about the responses generated in anomalous cognition experiments; in other words, there
      is no way to define what they would look like "by chance." Therefore, the random mechanism in these experiments
      must be in the choice of the target. In that way, we can compare the response to the target and answer the
      question: "If chance alone is at work, what is the probability that a target would be chosen that matches this
      response as well as or better than does the actual target?"</p>
    <p>In order to accomplish this purpose, a properly conducted experiment uses a set of targets defined in advance.
      The target for each remote viewing is then selected randomly, in such a way that the probability of getting each
      possible target is known.</p>
    <p>The SAIC remote viewing experiments and all but the early ones at SRI used a statistical evaluation method known
      as rank-order judging. After the completion of a remote viewing, a judge who is blind to the true target (called a
      blind judge) is shown the response and five potential targets, one of which is the correct answer and the other
      four of which are "decoys." Before the experiment is conducted each of those five choices must have had an equal
      chance of being selected as the actual target. The judge is asked to assign a rank to each of the possible
      targets, where a rank of one means it matches the response most closely, and a rank of five means it matches the
      least.</p>
    <p>The rank of the correct target is the numerical score for that remote viewing. By chance alone the actual target
      would receive each of the five ranks with equal likelihood, since despite what the response said the target
      matching it best would have the same chance of selection as the one matching it second best and so on. The average
      rank by chance would be three. Evidence for anomalous cognition occurs when the average rank over a series of
      trials is significantly lower than three. (Notice that a rank of one is the best possible score for each
      viewing.)</p>
    <p>This scoring method is conservative in the sense that it gives no extra credit for an excellent match. A response
      that describes the target almost perfectly will achieve the same rank of one as a response that contains only
      enough information to pick the target as the best choice out of the five possible choices. One advantage of this
      method is that it is still valid even if the viewer knows the set of possible targets. The probability of a first
      place match by chance would still be only one in five. This is important because the later SRI and many of the
      SAIC experiments used the same large set of National Geographic photographs as targets. Therefore, the experienced
      viewers would eventually become familiar with the range of possibilities since they were usually shown the answer
      at the end of each remote viewing session.</p>
    <p>For technical reasons explained in Appendix 1, the effect size for a series of remote viewings using rank-order
      judging with five choices is (3.0 - average rank)/√2. Therefore, small, medium and large effect sizes (0.2, 0.5
      and 0.8) correspond to average ranks of 2.72, 2.29 and 1.87, respectively. Notice that the largest effect size
      possible using this method is 1.4, which would result if every remote viewing achieved a first place ranking.</p>
  </section>
</section>
<section>
  <h3>Methodological Issues</h3>
  <p>One of the challenges in designing a good experiment in any area of science is to close the loopholes that would
    allow explanations other than the intended one to account for the results.</p>
  <p>There are a number of places in remote viewing experiment where information could be conveyed by normal means if
    proper precautions are not taken. The early SRI experiments suffered from some of those problems, but the later SRI
    experiments and the SAIC work were done with reasonable methodological rigor, with some exceptions noted in the
    detailed descriptions of the SAIC experiments in Appendix 2.</p>
  <p>The following list of methodological issues shows the variety of concerns that must be addressed. It should be
    obvious that a well-designed experiment requires careful thought and planning:</p>
  <ul>
    <li>No one who has knowledge of the specific target should have any contact with the viewer until after the response
      has been safely secured.</li>
    <li>No one who has knowledge of the specific target or even of whether or not the session was successful should have
      any contact with the judge until after that task has been completed.</li>
    <li>No one who has knowledge of the specific target should have access to the response until after the judging has
      been completed.</li>
    <li>Targets and decoys used in judging should be selected using a well-tested randomization device.</li>
    <li>Duplicate sets of targets photographs should be used, one during the experiment and one during the judging, so
      that no cues (like fingerprints) can be inserted onto the target that would help the judge recognize it.</li>
    <li>The criterion for stopping an experiment should be defined in advance so that it is not called to a halt when
      the results just happen to be favorable. Generally, that means specifying the number of trials in advance, but
      some statistical procedures require or allow other stopping rules. The important point is that the rule be defined
      in advance in such a way that there is no ambiguity about when to stop.</li>
    <li>Reasons, if any, for excluding data must be defined in advance and followed consistently, and should not be
      dependent on the data. For example, a rule specifying that a trial could be aborted if the viewer felt ill would
      be legitimate, but only if the trial was aborted before anyone involved in that decision knew the correct
      target.</li>
    <li>Statistical analyses to be used must be planned in advance of collecting the data so that a method most
      favorable to the data isn't selected post hoc. If multiple methods of analysis are used the corresponding
      conclusions must recognize that fact.</li>
  </ul>
</section>
<section>
  <h3>2.4 Prima Facie Evidence</h3>
  <p>According to Webster's Dictionary, in law prima facie evidence is "evidence having such a degree of probability
    that it must prevail unless the contrary be proved." There are a few examples of applied, non-laboratory remote
    viewings provided to the review team that would seem to meet that criterion for evidence. These are examples in
    which the sponsor or another government client asked for a single remote viewing of a site, known to the requestor
    in real time or in the future, and the viewer provided details far beyond what could be taken as a reasonable guess.
    Two such examples are given by May (1995) in which it appears that the results were so striking that they far exceed
    the phenomenon as observed in the laboratory. Using a post hoc analysis, Dr. May concluded that in one of the cases
    the remote viewer was able to describe a microwave generator with 80 percent accuracy, and that of what he said
    almost 70 percent of it was reliable. Laboratory remote viewings rarely show that level of correspondence.</p>
  <p>Notice that standard statistical methods cannot be used in these cases because there is no standard for
    probabilistic comparison. But evidence gained from applied remote viewing cannot be dismissed as inconsequential
    just because we cannot assign specific probabilities to the results. It is most important to ascertain whether or
    not the information was achievable in other standard ways. In Section 3 an example is given in which a remote viewer
    allegedly gave codewords from a secret facility that he should not have even known existed. Suppose the sponsors
    could be absolutely certain that the viewer could not have known about those codewords through normal means. Then
    even if we can't assign an exact probability to the fact that he guessed them correctly, we can agree that it would
    be very small. That would seem to constitute prima facie evidence unless an alternative explanation could be found.
    Similarly, the viewer who described the microwave generator allegedly knew only that the target was a technical site
    in the United States. Yet, he drew and described the microwave generator, including its function, its approximate
    size, how it was housed and that it had "a beam divergence angle of 30 degrees" (May, 1995, p. 15).</p>
  <p>Anecdotal reports of psychic functioning suffer from a similar problem in terms of their usefulness as proof. They
    have the additional difficulty that the "response" isn't even well- defined in advance, unlike in applied remote
    viewing where the viewer provides a fixed set of information on request. For instance, if a few people each night
    happen to dream of plane crashes, then some will obviously do so on the night before a major plane crash. Those
    individuals may interpret the coincidental timing as meaningful. This is undoubtedly the reason many people think
    the reality of psychic functioning is a matter of belief rather than science, since they are more familiar with the
    provocative anecdotes than with the laboratory evidence.</p></section>
<!--#include virtual="/footer.html" -->
