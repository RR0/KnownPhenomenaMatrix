<!--#include virtual="/header-start.html" -->
<title>Régression linéaire</title>
<!--#include virtual="/header-end.html" -->
<section>
  <h2>Motivation</h2>
  <ul>
    <li>Déterminer si une variable quantitative (y) à une influence sur une autre (x)</li>
    <li><strong>Prédire</strong> un résultat en fonction d'un ensemble de données connues</li>
  </ul>
</section>
<section>
  <h2>Analyse</h2>
  <figure class="right side">
    <figcaption>Exemple de régression linéaire à partir de points</figcaption>
    <img src="Linear_regression.svg" alt="Exemple de régression linéaire à partir de points"/>
  </figure>
  <p>Soit :</p>
  <ul>
    <li>`X` une matrice de `m xx n` caractéristiques (<em>features</em>), i.e. des lignes de données connues</li>
    <li>`y` un vecteur de `m` données résultantes connues</li>
  </ul>
  <p>Ce type de <a href="/science/discipline/math/stat/regress">régression</a> consiste à déterminer la fonction
    <strong>hypothèse</strong> `h_Θ(x) = Θ_0 + x_1Θ_1 + x_2Θ_2 + ... + x_nΘ_n` dont les points sont les moins éloignés
    des valeurs réelles. Autrement dit déterminer les <strong>paramètres</strong> `Θ_0, Θ_1, ..., Θ_n` qui permettent à
    la fonction linéaire `h` d'avoir des points ayant une distance/différence minimale avec les points "réels" (connus).
  </p>
  <p>Les cas les plus courants ne mobilisent cependant que 1 (`Θ_0 + xΘ_1`) ou 2 paramètres (`Θ_0 + x_1Θ_1 + x_2Θ_2`).
    S'il est besoin de représenter des fonctions plus complexes (typiquement si la fonction hypothèse ne correspond pas
    assez aux données ou <i lang="en">underfitting</i>), on recourra aux <a href="../non-linear">régression
      non-linéaires</a>.
  </p>
</section>
<section>
  <h2>Conception</h2>
  <p>On cherche à trouver les `Θ` (`Θ_0, Θ_1`...) optimaux selon une méthode adaptée :</p>
  <ul>
    <li><strong><a href="../normal">analytique</a></strong> si `n` est petit (< 10000) et `X` inversible.</li>
    <li><strong><a href="/science/discipline/math/stat/regress/gradient">itérative</a></strong> sinon.</li>
  </ul>
  <section>
    <h3>Méthode itérative</h3>
    <p>On fourni à l'algorithme de <a href="/science/discipline/math/stat/regress/gradient">Descente de gradient</a> une
      <a href="../cost">fonction de <strong>coût</strong></a> permettant de calculer la différence entre `h` et `y`</p>
    <section>
      <h4>Coût</h4>
      <p>Comme les erreurs peuvent être positives ou négatives et que nous ne sommes intéressés que par
        l'écart/distance, on élève cette erreur au carré afin de le garantir toujours positif <span class="note">On pourrait théoriquement utiliser une fonction de valeur absolue à la place mais
        cela transformerait le résultat</span>. On annule/compense ensuite la mise au carré des erreurs en multipliant
        par 1/2 (ce qui annulera la <a href="/science/discipline/math/ens/rel/func/deriv">dérivée</a> de `x^2` qui est
        `2x`) :</p>
      <p>`c = 1/2 (h_Θ - y)^2`</p>
      <p>On parlera ici aussi de "fonction d'erreur carrée" (<i lang="en">squared error function</i>).</p>
    </section>
    <section>
      <h4>Vectorisation</h4>
      <p>Afin de gagner en performance et simplicité d'écriture on peut réécrire la formule d'hypothèse sous forme de
        multiplication de matrices `Θ` et `x` (une ligne de `X`). Afin de conserver la constante `Θ_0`, on définira
        `x_0` = 1 :</p>
      <p>`Θ = [[Θ_0],[Θ_1],[Θ_2],[⋮],[Θ_n]]`, `x = [[x_0],[x_1],[x_2],[⋮],[x_n]]`</p>
      <p>Cependant on ne peut multiplier une matrice `1 xx (n + 1)` que par une matrice `(n + 1) xx 1`, et il faut donc
        transposer `Θ` pour obtenir le calcul attendu :</p>
      <p>`Θ^T = [Θ_0,Θ_1,Θ_2,...,Θ_n]`, `x = [[x_0],[x_1],[x_2],[⋮],[x_n]]`</p>
      <p>Ainsi :</p>
      <p>`h_Θ(x) = Θ^Tx`</p>U
      <p>`h_Θ(x) = x_0Θ_0 + x_1Θ_1 + x_2Θ_2 + ... + x_nΘ_n` (le calcul recherché)</p>
      <p>Cependant au niveau de l'ensemble du <i lang="en">training set</i> `X` (ou <i lang="en">design matrix</i>), les
        `x^((i))` ne sont pas des vecteurs mais des lignes, de sorte qu'on doive plutôt y stocker des `(x^((i)))^T`s :
      </p>
      <p>`X = [[(x^((0)))^T],[(x^((1)))^T],[⋮],[(x^((m)))^T]]`</p>
      <p>Comme le vecteur `x` est déjà transposé de par sa notation en ligne, `Θ` ne doit plus l'être pour que leur
        multiplication s'opère. Cela reste équivalent car les 2 étant des vecteurs, `Θ^Tx = x^TΘ`.</p>
      <p>Le calcul recherché peut alors être effectué pour toutes les lignes de la matrice en multipliant `X` par le
        vecteur `Θ` :</p>
      <p>`h_Θ = XΘ = [[(x^((0)))^TΘ],[(x^((1)))^TΘ],[⋮],[(x^((m)))^TΘ]]`</p>
    </section>
  </section>
</section>
<section>
  <h2>Notes</h2>
  <ul>
    <li>Avant tout travail de modélisation, une approche descriptive ou exploratoire est nécessaire pour dépister au
      plus tôt des difficultés dans les données : dissymétrie des distributions, valeurs atypiques, liaison non linéaire
      entre les variables.</li>
    <li>Le modèle suppose implicitement une notion préalable de <strong>causalité</strong> dans le sens où Y dépend de X
      car le modèle n’est pas symétrique (i.e. X ne dépend pas forcément de Y).</li>
  </ul>
</section>
<section>
  <h2>Exemples</h2>
  <p>En <a href="/tech/info/soft/data/science/ml">ML</a>, la régression linéaire permet de produire une fonction
    continue à partir de données connues, qui permet de prédire des valeurs probables pour d'autres valeurs de x ou y.
  </p>
</section>
<!--#include virtual="/footer.html" -->
<style>.mjx-math * {
  line-height: 0;
}</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=AM_CHTML"></script>
