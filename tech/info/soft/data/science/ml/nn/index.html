<!--#include virtual="/header-start.html" -->
<title>N. N.</title>
<!--#include virtual="/header-end.html" -->
<p><i lang="en">Neural network</i> : réseau de neurones</p>
<section>
  <h2>Motivation</h2>
  <p><a href="..">Apprendre</a> à partir de nombreux critères (trouver des hypothèses <a
      href="/science/discipline/math/stat/regress/non-linear">non-linéaires</a> complexes).</p>
</section>
<section>
  <h2>Analyse</h2>
  <p>Lorsque le nombre de critères est trop grand (reconnaissance d'images par ex), <a
      href="/science/discipline/math/stat/regress/non-linear">l'approche polynomiale</a> devient trop coûteuse en
    calcul et il devient plus intéressant d'utiliser une alternative comme un réseau de neurones.</p>
  <p>À l'image des <a href="/science/discipline/bio/anat/nerv/brain/neuro">neurones biologiques</a>, un neurone
    artificiel possède :</p>
  <ul>
    <li>des entrées :
      <ul>
        <li>`x_0 = 1`, l'unité de biais</li>
        <li>`x_1, x_2, ..., x_n`</li>
      </ul>
    <li>une sortie calculée par la fonction de prédiction `h_Θ(x)` où Θ est une série de paramètres (ou "poids")</li>
  </ul>
  <section>
    <h3>Réseau</h3>
    <p>Un réseau est constitué de `L` couches (souvent représentées de gauche à droite) :</p>
    <ol>
      <li>couche de `s_1` entrées</li>
      <li>`L-2` couches "cachées" : contient `s_n` <strong>unités</strong> d'activation (noeuds) `a_0^i, a_1^i, ...,
        a_n^i`</li>
      <li>couche de `s_L` sorties</li>
    </ol>
    <p>Par exemple pour 1 seule couche "cachée" (2) :</p>
    <p>`[[x_0],[x_1],[x_2],[x_3]] -> [[a_1^((2))],[a_2^((2))],[a_3^((2))]] -> hθ(x)`</p>
  </section>
</section>
<section>
  <h2>Conception</h2>
  <p>On définit pour chacun des `n` noeuds d'une couche `j` un résultat dépendant de la couche précédente `j-1` et de sa
    matrice de poids `Θ_j` :</p>
  <p>`z_i^((j))=Θ_(i,0)^((j-1)) x_0 + Θ_(i,1)^((j-1)) x_1 + Θ_(i,2)^((j-1)) x_2 + Θ_(i,3)^((j-1)) x_3`</p>
  <p>et l'on définit alors la <strong>fonction d'activation</strong> <a
      href="/science/discipline/math/stat/Sigmoide.html">logistique</a> `g` comme :</p>
  <p>`a_i^((j))=g(z_i^((j)))`</p>
  <p>et l'on considère la sortie `h_Θ(x)` comme `a_1^(3)` par exemple s'il y a 3 couches, recevant la couche 2 comme X
    (i.e. `x_i = a_i`) :</p>
  <p>`h_Θ(x)=g(Θ_(10)^((2)) a_0 + Θ_(11)^((2)) a_1 + Θ_(12)^((2)) a_2 + Θ_(13)^((2)) a_3)`</p>
  <p>Toutefois les résultats peuvent être plus complexes. Dans des problèmes de classification multiple (plus de 2
    classes) par exemple, les résultats connus (et donc les hypothèses) auront plutôt la forme d'une matrice (où chaque
    ligne indique si la classe i est reconnue ou non par exemple) :</p>
  <p>`h_Θ(x)=[[h_Θ(x)_1],[h_Θ(x)_2],[h_Θ(x)_3],[h_Θ(x)_4]]`</p>
  <section>
    <h3>Prédiction</h3>
    <p>Comme nous disposons potentiellement de `K` sorties, nous devons minimiser le coût de `(h_Θ(x))_i`
      (i<sup>ème </sup> sortie) via une <a
          href="/science/discipline/math/stat/regress/cost">fonction de coût</a> similaire à celle utilisée pour la <a
          href="/science/discipline/math/stat/regress/logistic">régression logistique</a>, mais pour `K` sorties (`y`).
      Idem pour la <a href="/">fonction de régularisation</a>.
    </p>
  </section>
</section>
<section>
  <h2>Exemples</h2>
  <section>
    <h3>"Et" logique</h3>
    <p>On définit des paramètres/poids générant la table de vérité du "et" logique :</p>
    <p>`Θ^((1))=[−30, 20, 20]`</p>
    <p>ce qui donne :</p>
    <p>`h_Θ(x)=g(−30 + 20 x_1 + 20 x_2)`</p>
    <p>et donc :</p>
    <ul>
      <li>si `x_1 = 0` et `x_2 = 0` alors `g(-30 + 0 + 0) = g(-30) ~~ 0`</li>
      <li>si `x_1 = 0` et `x_2 = 1` alors `g(-30 + 0 + 20) = g(-10) ~~ 0`</li>
      <li>si `x_1 = 1` et `x_2 = 0` alors `g(-30 + 20 + 0) = g(-10) ~~ 0`</li>
      <li>si `x_1 = 1` et `x_2 = 1` alors `g(-30 + 20 + 20) = g(10) ~~ 1`</li>
    </ul>
    <p>(puisque <a
        href="/science/discipline/math/stat/Sigmoide.html">`g`</a> permet de produire des valeurs tendant vers 0 ou 1
      suivant que x est négatif ou positif)</p>
  </section>
  <section>
    <h3>"Ou" logique</h3>
    <p>On définit des paramètres/poids générant la table de vérité du "ou" logique :</p>
    <p>`Θ^((1))=[−10, 20, 20]`</p>
    <p>ce qui donne :</p>
    <p>`h_Θ(x)=g(−10 + 20 x_1 + 20 x_2)`</p>
    <p>et donc :</p>
    <ul>
      <li>si `x_1 = 0` et `x_2 = 0` alors `g(-10 + 0 + 0) = g(-10) ~~ 0`</li>
      <li>si `x_1 = 0` et `x_2 = 1` alors `g(-10 + 0 + 20) = g(10) ~~ 1`</li>
      <li>si `x_1 = 1` et `x_2 = 0` alors `g(-10 + 20 + 0) = g(10) ~~ 1`</li>
      <li>si `x_1 = 1` et `x_2 = 1` alors `g(-10 + 20 + 20) = g(30) ~~ 1`</li>
    </ul>
  </section>
  <section>
    <h3>"Non" logique</h3>
    <p>On définit des paramètres/poids générant la table de vérité du "non" logique :</p>
    <p>`Θ^((1))=[10, -20]`</p>
    <p>ce qui donne :</p>
    <p>`h_Θ(x)=g(10 - 20 x_1)`</p>
    <p>et donc :</p>
    <ul>
      <li>si `x_1 = 0` alors `g(10 - 0) = g(10) ~~ 1`</li>
      <li>si `x_1 = 1` alors `g(10 - 20) = g(-10) ~~ 0`</li>
    </ul>
  </section>
  <section>
    <h3>"Nor" logique</h3>
    <p>On définit des paramètres/poids générant la table de vérité du "ni l'un ni l'autre" (`x_1 = x_2 = 0`) logique
      :</p>
    <p>`Θ^((1))=[10, -20, -20]`</p>
    <p>ce qui donne :</p>
    <p>`h_Θ(x)=g(10 - 20 x_1 - 20 x_2)`</p>
    <p>et donc :</p>
    <ul>
      <li>si `x_1 = 0` et `x_2 = 0` alors `g(10 - 0 - 0) = g(10) ~~ 1`</li>
      <li>si `x_1 = 0` et `x_2 = 1` alors `g(10 - 20 - 0) = g(-10) ~~ 0`</li>
      <li>si `x_1 = 1` et `x_2 = 0` alors `g(10 - 0 - 20) = g(-10) ~~ 0`</li>
      <li>si `x_1 = 1` et `x_2 = 1` alors `g(10 - 20 - 20) = g(-30) ~~ 0`</li>
    </ul>
  </section>
  <section>
    <h3>Not(Xor)</h3>
    <p>On réutilise les paramètres/poids des exemples précédents :</p>
    <ul>
      <li>1ère couche :
        <ul>
          <li>Le "et" : `Θ^((1))=[−30, 20, 20]`</li>
          <li>Le "nor" : `Θ^((1))=[10, -20, -20]`</li>
        </ul>
        donnent : `Θ^((1))=[[−30, 20, 20],[10, -20, -20]]`
      </li>
      <li>2ème couche : Le "ou" : `Θ^((2))=[-10, 20, 20]`</li>
    </ul>
    <p>on calcule alors le résultat des couches :</p>
    <p>`a^((2))=g(Θ^((1)) * x)`</p>
    <p>`a^((3))=g(Θ^((2)) * a^((2)))`</p>
    <p>ce qui donne :</p>
    <p>`h_Θ(x)=a^((3))`</p>
  </section>
</section>
<section>
  <h2>Notes</h2>
  <ul>
    <li>Inspiré des <a href="/science/discipline/bio/anat/nerv/brain/neuro">neurones biologiques</a>.
    </li>
  </ul>
</section>
<!--#include virtual="/footer.html" -->
<style>.mjx-math * {
  line-height: 0;
}</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=AM_CHTML"></script>