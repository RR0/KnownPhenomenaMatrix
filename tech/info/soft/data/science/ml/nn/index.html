<!--#include virtual="/header-start.html" -->
<title>N. N.</title>
<!--#include virtual="/header-end.html" -->
<p><i lang="en">Neural network</i> : réseau de neurones</p>
<section>
  <h2>Motivation</h2>
  <p><a href="..">Apprendre</a> à partir de nombreux critères (trouver des hypothèses <a
      href="/science/discipline/math/stat/regress/non-linear">non-linéaires</a> complexes).</p>
</section>
<section>
  <h2>Analyse</h2>
  <p>Lorsque le nombre de critères est trop grand (reconnaissance d'images par ex), <a
      href="/science/discipline/math/stat/regress/non-linear">l'approche polynomiale</a> devient trop coûteuse en calcul
    et il devient plus intéressant d'utiliser une alternative comme un réseau de neurones.</p>
  <p>À l'image des <a href="/science/discipline/bio/anat/nerv/brain/neuro">neurones biologiques</a>, un neurone
    artificiel possède :</p>
  <ul>
    <li>des entrées :
      <ul>
        <li>`x_0 = 1`, l'unité de biais</li>
        <li>`x_1, x_2, ..., x_n`</li>
      </ul>
    <li>une sortie calculée par la fonction de prédiction `h_Θ(x)` où Θ est une série de paramètres (ou "poids")</li>
  </ul>
  <section>
    <h3>Réseau</h3>
    <p>Un réseau est constitué de `L` couches (souvent représentées de gauche à droite) :</p>
    <ol>
      <li>couche de `s_1` entrées</li>
      <li>`L-2` couches "cachées" : contient `s_n` <strong>unités</strong> d'activation (noeuds) `a_0^i, a_1^i, ...,
        a_n^i`</li>
      <li>couche de `s_L` sorties</li>
    </ol>
    <p>Par exemple pour 1 seule couche "cachée" (2) :</p>
    <p>`[[x_0],[x_1],[x_2],[x_3]] -> [[a_1^((2))],[a_2^((2))],[a_3^((2))]] -> hθ(x)`</p>
  </section>
</section>
<section>
  <h2>Conception</h2>
  <p>On définit pour chacun des `n` noeuds d'une couche `j` un résultat dépendant de la couche précédente `j-1` et de sa
    matrice de poids `Θ_j` :</p>
  <p>`z_i^((j))=Θ_(i,0)^((j-1)) x_0 + Θ_(i,1)^((j-1)) x_1 + Θ_(i,2)^((j-1)) x_2 + Θ_(i,3)^((j-1)) x_3`</p>
  <p>et l'on définit alors la <strong>fonction d'activation</strong> comme une fonction <a
      href="/science/discipline/math/stat/Sigmoide.html">logistique</a> `g` :</p>
  <p>`a_i^((j))=g(z_i^((j)))`</p>
  <p>et l'on considère la sortie `h_Θ(x)` comme `a_1^(3)` par exemple (s'il y a 3 couches), recevant la couche 2 comme
    `X` (i.e. `x_i = a_i`) :</p>
  <p>`h_Θ(x)=g(Θ_(1,0)^((2)) a_0 + Θ_(1,1)^((2)) a_1 + Θ_(1,2)^((2)) a_2 + Θ_(1,3)^((2)) a_3)`</p>
  <p>Toutefois les résultats peuvent être plus complexes. Dans des problèmes de classification multiple (plus de 2
    classes) par exemple, les résultats connus (et donc les hypothèses) auront plutôt la forme d'une matrice (où chaque
    ligne indique si la classe `i` est reconnue ou non par exemple) :</p>
  <p>`h_Θ(x)=[[h_Θ(x)_1],[h_Θ(x)_2],[h_Θ(x)_3],[h_Θ(x)_4]]`</p>
  <p>On parlera donc plus généralement de `h_Θ(x)_k` comme étant le `k`<sup>ième</sup> résultat dans la couche de sortie
    .</p>
  <section>
    <h3>Prédiction</h3>
    <p>Comme pour d'autres algorithmes de <a href="/tech/info/soft/data/science/ml">ML</a>, avant de prédire, le réseau
      doit être "entraîné" (<i lang="en">trained </i>) en 2 phases à répéter jusqu'à convergence :</p>
    <ol>
      <li><strong>Propagation avant</strong> (<i lang="en">forward propagation</i>) pour calcul du coût (erreur de
        prédiction)</li>
      <li><strong>Rétro-propagation</strong> (<i lang="en">back propagation</i>) pour mise à jour des paramètres en
        conséquence.</li>
    </ol>
    <section>
      <h4>Propagation avant</h4>
      <p>La propagation avant consiste à calculer :</p>
      <ul>
        <li>pour chaque ligne de données (`i` de 1 à `m`)
          <ul>
            <li>pour chaque feature (`j` de 0 à `n` en incluant la feature de biais n° 0)
              <ul>
                <li>sa valeur d'activation `a_j^((i)) = g(z_j^((i)))` où `z_j^((i))` est la somme des noeuds (toujours
                  en commençant par le noeud de biais d'indice 0) de la couche précédente en entrée multipliés par leurs
                  poids `Θ_(i,j)^((i))`.</li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
      <section>
        <h5>Coût</h5>
        <p>Pour calculer le coût pour un réseau neuronal, on généralise celui de la <a
            href="/science/discipline/math/stat/regress/logistic">régression logistique</a> non pas pour une seule
          sortie y mais pour `K` sorties en insérant une somme supplémentaire pour tenir compte des `K` noeuds de sortie
          ainsi que des `S_l` unités (y compris le noeud de biais) de chaque couche :</p>
        <p>`J(Θ) = -1/m sum_(i=1)^m sum_(k=1)^K [y_k^((i)) log(h_θ(x^((i)))_k)+(1−y_k^((i)))log(1−h_θ(x^((i)))_k)] +
          λ/(2m) sum_(l=1)^(L-1) sum_(i=1)^(S_l) sum_(j=1)^(S_l+1) (θ_(j,i)^((l)))^2`</p>
        <p>Pour minimiser ce coût, nous allons chercher à minimiser la <a
            href="/science/discipline/math/ens/rel/func/deriv">dérivée</a> de cette fonction (i.e. plus la pente du coût
          est faible, plus on s'approche de la solution).</p>
        <p>Comme nous disposons potentiellement de `K` sorties, nous devons minimiser le coût de `(h_Θ(x))_i`
          (i<sup>ème</sup> sortie) via une <a
              href="/science/discipline/math/stat/regress/cost">fonction de coût</a> similaire à celle utilisée pour la
          <a
              href="/science/discipline/math/stat/regress/logistic">régression logistique</a>, mais pour `K` sorties
          (`y`). Idem pour la <a href="/">fonction de régularisation</a>.</p>

      </section>
    </section>
    <section>
      <h4>Rétro-propagation</h4>
      <p>La rétro-propagation</p>
    </section>
  </section>
</section>
<section>
  <h2>Exemples</h2>
  <section>
    <h3>"Et" logique</h3>
    <p>On définit des paramètres/poids générant la table de vérité du "et" logique :</p>
    <p>`Θ^((1))=[−30, 20, 20]`</p>
    <p>ce qui donne :</p>
    <p>`h_Θ(x)=g(−30 + 20 x_1 + 20 x_2)`</p>
    <p>et donc :</p>
    <ul>
      <li>si `x_1 = 0` et `x_2 = 0` alors `g(-30 + 0 + 0) = g(-30) ~~ 0`</li>
      <li>si `x_1 = 0` et `x_2 = 1` alors `g(-30 + 0 + 20) = g(-10) ~~ 0`</li>
      <li>si `x_1 = 1` et `x_2 = 0` alors `g(-30 + 20 + 0) = g(-10) ~~ 0`</li>
      <li>si `x_1 = 1` et `x_2 = 1` alors `g(-30 + 20 + 20) = g(10) ~~ 1`</li>
    </ul>
    <p>(puisque <a
        href="/science/discipline/math/stat/Sigmoide.html">`g`</a> permet de produire des valeurs tendant vers 0 ou 1
      suivant que x est négatif ou positif)</p>
  </section>
  <section>
    <h3>"Ou" logique</h3>
    <p>On définit des paramètres/poids générant la table de vérité du "ou" logique :</p>
    <p>`Θ^((1))=[−10, 20, 20]`</p>
    <p>ce qui donne :</p>
    <p>`h_Θ(x)=g(−10 + 20 x_1 + 20 x_2)`</p>
    <p>et donc :</p>
    <ul>
      <li>si `x_1 = 0` et `x_2 = 0` alors `g(-10 + 0 + 0) = g(-10) ~~ 0`</li>
      <li>si `x_1 = 0` et `x_2 = 1` alors `g(-10 + 0 + 20) = g(10) ~~ 1`</li>
      <li>si `x_1 = 1` et `x_2 = 0` alors `g(-10 + 20 + 0) = g(10) ~~ 1`</li>
      <li>si `x_1 = 1` et `x_2 = 1` alors `g(-10 + 20 + 20) = g(30) ~~ 1`</li>
    </ul>
  </section>
  <section>
    <h3>"Non" logique</h3>
    <p>On définit des paramètres/poids générant la table de vérité du "non" logique :</p>
    <p>`Θ^((1))=[10, -20]`</p>
    <p>ce qui donne :</p>
    <p>`h_Θ(x)=g(10 - 20 x_1)`</p>
    <p>et donc :</p>
    <ul>
      <li>si `x_1 = 0` alors `g(10 - 0) = g(10) ~~ 1`</li>
      <li>si `x_1 = 1` alors `g(10 - 20) = g(-10) ~~ 0`</li>
    </ul>
  </section>
  <section>
    <h3>"Nor" logique</h3>
    <p>On définit des paramètres/poids générant la table de vérité du "ni l'un ni l'autre" (`x_1 = x_2 = 0`) logique
      :</p>
    <p>`Θ^((1))=[10, -20, -20]`</p>
    <p>ce qui donne :</p>
    <p>`h_Θ(x)=g(10 - 20 x_1 - 20 x_2)`</p>
    <p>et donc :</p>
    <ul>
      <li>si `x_1 = 0` et `x_2 = 0` alors `g(10 - 0 - 0) = g(10) ~~ 1`</li>
      <li>si `x_1 = 0` et `x_2 = 1` alors `g(10 - 20 - 0) = g(-10) ~~ 0`</li>
      <li>si `x_1 = 1` et `x_2 = 0` alors `g(10 - 0 - 20) = g(-10) ~~ 0`</li>
      <li>si `x_1 = 1` et `x_2 = 1` alors `g(10 - 20 - 20) = g(-30) ~~ 0`</li>
    </ul>
  </section>
  <section>
    <h3>Not(Xor)</h3>
    <p>On réutilise les paramètres/poids des exemples précédents :</p>
    <ul>
      <li>1ère couche :
        <ul>
          <li>Le "et" : `Θ^((1))=[−30, 20, 20]`</li>
          <li>Le "nor" : `Θ^((1))=[10, -20, -20]`</li>
        </ul>
        donnent : `Θ^((1))=[[−30, 20, 20],[10, -20, -20]]`
      </li>
      <li>2ème couche : Le "ou" : `Θ^((2))=[-10, 20, 20]`</li>
    </ul>
    <p>on calcule alors le résultat des couches :</p>
    <p>`a^((2))=g(Θ^((1)) * x)`</p>
    <p>`a^((3))=g(Θ^((2)) * a^((2)))`</p>
    <p>ce qui donne :</p>
    <p>`h_Θ(x)=a^((3))`</p>
  </section>
</section>
<section>
  <h2>Notes</h2>
  <ul>
    <li>Inspiré des <a href="/science/discipline/bio/anat/nerv/brain/neuro">neurones biologiques</a>.
    </li>
  </ul>
</section>
<section>
  <h2>Voir</h2>
  <ul>
    <li>Moawad, Assaad: "<a
        href="https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e">Neural
      networks and back-propagation explained in a simple way</a>", Medium, 2018-02-01</li>
  </ul>
</section>
<!--#include virtual="/footer.html" -->
<style>.mjx-math * {
  line-height: 0;
}</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=AM_CHTML"></script>